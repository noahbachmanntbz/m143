# Tagesdokumentation — 26.08.2025

Kurzfassung: Aufbau der Grundlagen für ein automatisiertes Backup-System mit AWS S3. Bucket erstellt, Versionierung und Lifecycle-Policy konfiguriert, erste Backup-Skripte implementiert und per Cronjob automatisiert. Test-Uploads erfolgreich durchgeführt und validiert.

---

## ✅ Erledigte Arbeiten

- AWS CLI auf lokalem Rechner eingerichtet und konfiguriert
- S3-Bucket für Backups erstellt mit Versionierung
- Lifecycle-Policy implementiert (30d → Glacier, 90d → Delete)
- Backup-Skript erstellt und auf EC2-Instanz eingerichtet
- Cronjob für tägliche automatische Backups konfiguriert
- Test-Uploads durchgeführt und validiert

---

## 🔧 Einrichtung der AWS CLI
Zuerst wurde die AWS CLI auf dem lokalen Rechner eingerichtet und mit temporären Zugangsdaten konfiguriert:

```powershell
# Zugangsdaten als Umgebungsvariablen setzen
$env:AWS_ACCESS_KEY_ID="..."
$env:AWS_SECRET_ACCESS_KEY="..."
$env:AWS_SESSION_TOKEN="..."
$env:AWS_DEFAULT_REGION="us-east-1"

# Test der Verbindung
aws sts get-caller-identity
```

---

## 📦 Erstellung des S3-Buckets
In der AWS CloudShell wurde ein eigener Bucket für die Backups erstellt.  

```bash
# Bucket-Namen definieren
BUCKET=backup-raw-bachmann-pe24c

# Bucket erstellen
aws s3api create-bucket --bucket $BUCKET

# Versioning aktivieren
aws s3api put-bucket-versioning   --bucket $BUCKET   --versioning-configuration Status=Enabled
```

---

## ⚙️ Lifecycle-Konfiguration
Damit alte Backups automatisch archiviert oder gelöscht werden, wurde eine Lifecycle-Policy erstellt.

```bash
cat > lifecycle.json <<'JSON'
{
  "Rules": [ {
    "ID": "ArchiveAndExpire",
    "Status": "Enabled",
    "Filter": {}, 
    "Transitions": [ {
      "Days": 30,
      "StorageClass": "GLACIER"
    } ],
    "Expiration": {
      "Days": 90
    }
  } ]
}
JSON

aws s3api put-bucket-lifecycle-configuration   --bucket $BUCKET   --lifecycle-configuration file://lifecycle.json
```

Überprüfung der Regel:
```bash
aws s3api get-bucket-lifecycle-configuration --bucket $BUCKET
```

---

## 🔎 Test-Upload
Ein Testfile wurde erzeugt und in den S3-Bucket hochgeladen.

```bash
echo "hello from learner lab" > /tmp/test.txt
aws s3 cp /tmp/test.txt s3://$BUCKET/backups/raw/test.txt
```

Überprüfung:
```bash
aws s3 ls s3://$BUCKET/backups/raw/
```

---

## 🤖 Automatisierte Backups mit Cron
Auf der EC2-Instanz wurde ein Script für tägliche Backups erstellt (`/opt/backup/daily_backup.sh`).  
Es erstellt ein inkrementelles Tar-Archiv und lädt es nach S3 hoch.  

### Script-Inhalt:
```bash
#!/usr/bin/env bash
set -euo pipefail
source /opt/backup/backup.env
TS=$(date -u +%Y%m%dT%H%M%SZ)
LOG=/var/backups/logs/daily-$TS.log
exec > >(tee -a "$LOG") 2>&1

STATE=/var/backups/state/files.snar
ARCH=/var/backups/tmp/files-$TS.tar.gz

# Backup von /etc (und optional /var/www, falls vorhanden)
TARGETS="/etc"
[ -d /var/www ] && TARGETS="$TARGETS /var/www"

sudo tar --listed-incremental="$STATE" -czf "$ARCH" $TARGETS   --ignore-failed-read --warning=no-file-changed

aws s3 cp "$ARCH" s3://$S3_BUCKET/backups/raw/files-$TS.tar.gz --region "$AWS_REGION"
rm -f "$ARCH"
```

Das Script wurde ausführbar gemacht:
```bash
sudo chmod +x /opt/backup/daily_backup.sh
```

Ein Cronjob wurde eingerichtet, damit das Backup täglich um 02:00 Uhr läuft:
```bash
( crontab -l 2>/dev/null; echo "0 2 * * * /opt/backup/daily_backup.sh" ) | crontab -
```

---

## ✔️ Prüfung der Backups
Alle gespeicherten Backups im S3-Bucket anzeigen:
```bash
aws s3 ls s3://$BUCKET/backups/raw/ --region us-east-1
```

---

## 🧾 Notizen / Referenzen

- AWS CLI erfolgreich konfiguriert mit temporären Zugangsdaten aus Learner Lab
- S3-Bucket: `backup-raw-bachmann-pe24c`
- Region: `us-east-1`
- Backup-Skript: `/opt/backup/daily_backup.sh`
- Cronjob: täglich um 02:00 Uhr
- Lifecycle: 30d → Glacier, 90d → Delete
- RTO: maximal 2 Stunden (Neustart oder Restore aus S3)
- RPO: höchstens 24 Stunden (tägliche Dumps in S3)