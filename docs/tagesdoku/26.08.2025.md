# Tagesdokumentation â€” 26.08.2025

Kurzfassung: Aufbau der Grundlagen fÃ¼r ein automatisiertes Backup-System mit AWS S3. Bucket erstellt, Versionierung und Lifecycle-Policy konfiguriert, erste Backup-Skripte implementiert und per Cronjob automatisiert. Test-Uploads erfolgreich durchgefÃ¼hrt und validiert.

---

## âœ… Erledigte Arbeiten

- AWS CLI auf lokalem Rechner eingerichtet und konfiguriert
- S3-Bucket fÃ¼r Backups erstellt mit Versionierung
- Lifecycle-Policy implementiert (30d â†’ Glacier, 90d â†’ Delete)
- Backup-Skript erstellt und auf EC2-Instanz eingerichtet
- Cronjob fÃ¼r tÃ¤gliche automatische Backups konfiguriert
- Test-Uploads durchgefÃ¼hrt und validiert

---

## ğŸ”§ Einrichtung der AWS CLI
Zuerst wurde die AWS CLI auf dem lokalen Rechner eingerichtet und mit temporÃ¤ren Zugangsdaten konfiguriert:

```powershell
# Zugangsdaten als Umgebungsvariablen setzen
$env:AWS_ACCESS_KEY_ID="..."
$env:AWS_SECRET_ACCESS_KEY="..."
$env:AWS_SESSION_TOKEN="..."
$env:AWS_DEFAULT_REGION="us-east-1"

# Test der Verbindung
aws sts get-caller-identity
```

---

## ğŸ“¦ Erstellung des S3-Buckets
In der AWS CloudShell wurde ein eigener Bucket fÃ¼r die Backups erstellt.  

```bash
# Bucket-Namen definieren
BUCKET=backup-raw-bachmann-pe24c

# Bucket erstellen
aws s3api create-bucket --bucket $BUCKET

# Versioning aktivieren
aws s3api put-bucket-versioning   --bucket $BUCKET   --versioning-configuration Status=Enabled
```

---

## âš™ï¸ Lifecycle-Konfiguration
Damit alte Backups automatisch archiviert oder gelÃ¶scht werden, wurde eine Lifecycle-Policy erstellt.

```bash
cat > lifecycle.json <<'JSON'
{
  "Rules": [ {
    "ID": "ArchiveAndExpire",
    "Status": "Enabled",
    "Filter": {}, 
    "Transitions": [ {
      "Days": 30,
      "StorageClass": "GLACIER"
    } ],
    "Expiration": {
      "Days": 90
    }
  } ]
}
JSON

aws s3api put-bucket-lifecycle-configuration   --bucket $BUCKET   --lifecycle-configuration file://lifecycle.json
```

ÃœberprÃ¼fung der Regel:
```bash
aws s3api get-bucket-lifecycle-configuration --bucket $BUCKET
```

---

## ğŸ” Test-Upload
Ein Testfile wurde erzeugt und in den S3-Bucket hochgeladen.

```bash
echo "hello from learner lab" > /tmp/test.txt
aws s3 cp /tmp/test.txt s3://$BUCKET/backups/raw/test.txt
```

ÃœberprÃ¼fung:
```bash
aws s3 ls s3://$BUCKET/backups/raw/
```

---

## ğŸ¤– Automatisierte Backups mit Cron
Auf der EC2-Instanz wurde ein Script fÃ¼r tÃ¤gliche Backups erstellt (`/opt/backup/daily_backup.sh`).  
Es erstellt ein inkrementelles Tar-Archiv und lÃ¤dt es nach S3 hoch.  

### Script-Inhalt:
```bash
#!/usr/bin/env bash
set -euo pipefail
source /opt/backup/backup.env
TS=$(date -u +%Y%m%dT%H%M%SZ)
LOG=/var/backups/logs/daily-$TS.log
exec > >(tee -a "$LOG") 2>&1

STATE=/var/backups/state/files.snar
ARCH=/var/backups/tmp/files-$TS.tar.gz

# Backup von /etc (und optional /var/www, falls vorhanden)
TARGETS="/etc"
[ -d /var/www ] && TARGETS="$TARGETS /var/www"

sudo tar --listed-incremental="$STATE" -czf "$ARCH" $TARGETS   --ignore-failed-read --warning=no-file-changed

aws s3 cp "$ARCH" s3://$S3_BUCKET/backups/raw/files-$TS.tar.gz --region "$AWS_REGION"
rm -f "$ARCH"
```

Das Script wurde ausfÃ¼hrbar gemacht:
```bash
sudo chmod +x /opt/backup/daily_backup.sh
```

Ein Cronjob wurde eingerichtet, damit das Backup tÃ¤glich um 02:00 Uhr lÃ¤uft:
```bash
( crontab -l 2>/dev/null; echo "0 2 * * * /opt/backup/daily_backup.sh" ) | crontab -
```

---

## âœ”ï¸ PrÃ¼fung der Backups
Alle gespeicherten Backups im S3-Bucket anzeigen:
```bash
aws s3 ls s3://$BUCKET/backups/raw/ --region us-east-1
```

---

## ğŸ§¾ Notizen / Referenzen

- AWS CLI erfolgreich konfiguriert mit temporÃ¤ren Zugangsdaten aus Learner Lab
- S3-Bucket: `backup-raw-bachmann-pe24c`
- Region: `us-east-1`
- Backup-Skript: `/opt/backup/daily_backup.sh`
- Cronjob: tÃ¤glich um 02:00 Uhr
- Lifecycle: 30d â†’ Glacier, 90d â†’ Delete
- RTO: maximal 2 Stunden (Neustart oder Restore aus S3)
- RPO: hÃ¶chstens 24 Stunden (tÃ¤gliche Dumps in S3)